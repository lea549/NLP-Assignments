{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Write a python program to find out the words after '@' from the below sentences with the use of regex.\n",
    "\n",
    "\"xyz@gmail.com\",\n",
    "\"abc@yahoo.com\",\n",
    "\"xyz@hotmail.com\",\n",
    "\"abc@ineuron.ai\",\n",
    "\"xyz@outlook.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original ids\n",
      "['xyz@gmail.com', 'abc@yahoo.com', 'xyz@hotmail.com', 'abc@ineuron.ai', 'xyz@outlook.com']\n",
      "\n",
      "words after @\n",
      "gmail.com\n",
      "yahoo.com\n",
      "hotmail.com\n",
      "ineuron.ai\n",
      "outlook.com\n"
     ]
    }
   ],
   "source": [
    "ids=[\"xyz@gmail.com\",\n",
    "\"abc@yahoo.com\",\n",
    "\"xyz@hotmail.com\",\n",
    "\"abc@ineuron.ai\",\n",
    "\"xyz@outlook.com\"]\n",
    "\n",
    "print (\"original ids\")\n",
    "print(ids)\n",
    "\n",
    "import re\n",
    "print(\"\\nwords after @\")\n",
    "for i in ids:\n",
    "    # re.search() - searches the whole string and return the first matched occurence \n",
    "    # \\S - any non white space character (alphanumeric and special characters)\n",
    "    print(re.search(r'@\\S+',i).group()[1:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Write a python program with the use of regex to take out the word \"New\" from the following sentence.\n",
    "\n",
    "[\"New Delhi is the capital of India\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Delhi is the capital of India'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'New', \"\", \"New Delhi is the capital of India\") # New is substituted with empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** Create one python program in which you have to lowercase the sentence first and \n",
    "than delete digits from the following sentence.\n",
    "\n",
    "\"In India, 184 people got affected with Corona virus and 4 are died.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in india, people got affected with corona virus and are died.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "updated_sent=[]\n",
    "\n",
    "# splitting at each whitespace\n",
    "for i in re.split(r'\\s', \"In India, 184 people got affected with Corona virus and 4 are died.\".lower()):\n",
    "    if re.search(r'\\D+',i)  != None :\n",
    "        updated_sent.append(re.search(r'\\D+',i).group()) # taking only nondigit characters\n",
    "    else :\n",
    "        continue\n",
    "' '.join(updated_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** Do stemming, lemmatization and tokenization from the following sentence.\n",
    "\n",
    "\"I hope that, when I have built up my savings, I will be able to travel to Hawai.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence\n",
      "I hope that, when I have built up my savings, I will be able to travel to Hawai.\n",
      "\n",
      "word tokenization o/p:\n",
      "['I', 'hope', 'that', ',', 'when', 'I', 'have', 'built', 'up', 'my', 'savings', ',', 'I', 'will', 'be', 'able', 'to', 'travel', 'to', 'Hawai', '.']\n",
      "\n",
      "after removing punctuations:\n",
      "['I', 'hope', 'that', 'when', 'I', 'have', 'built', 'up', 'my', 'savings', 'I', 'will', 'be', 'able', 'to', 'travel', 'to', 'Hawai']\n",
      "\n",
      "Stemming\n",
      "-----------\n",
      "PorterStemmer\n",
      "['I', 'hope', 'that', 'when', 'I', 'have', 'built', 'up', 'my', 'save', 'I', 'will', 'be', 'abl', 'to', 'travel', 'to', 'hawai']\n",
      "LancasterStemmer\n",
      "['i', 'hop', 'that', 'when', 'i', 'hav', 'built', 'up', 'my', 'sav', 'i', 'wil', 'be', 'abl', 'to', 'travel', 'to', 'hawa']\n",
      "SnowballStemmer\n",
      "['i', 'hope', 'that', 'when', 'i', 'have', 'built', 'up', 'my', 'save', 'i', 'will', 'be', 'abl', 'to', 'travel', 'to', 'hawai']\n",
      "\n",
      "Lemmatization\n",
      "--------------\n",
      "['I', 'hope', 'that', 'when', 'I', 'have', 'build', 'up', 'my', 'save', 'I', 'will', 'be', 'able', 'to', 'travel', 'to', 'Hawai']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "porter=PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "snowball= SnowballStemmer(\"english\")\n",
    "sent=\"I hope that, when I have built up my savings, I will be able to travel to Hawai.\"\n",
    "print(\"Original sentence\")\n",
    "print(sent)\n",
    "\n",
    "# tokenization\n",
    "print(\"\\nword tokenization o/p:\")\n",
    "print(nltk.word_tokenize(sent))\n",
    "    \n",
    "# remove punctuation\n",
    "import string\n",
    "punct=string.punctuation\n",
    "rem_punct= [word for word in nltk.word_tokenize(sent) if not word in punct]\n",
    "print(\"\\nafter removing punctuations:\")\n",
    "print(rem_punct)\n",
    "\n",
    "# Stemming - bring word to stem form (may or may not have a meaning)\n",
    "print(\"\\nStemming\")\n",
    "print(\"-----------\")\n",
    "print(\"PorterStemmer\")\n",
    "print([porter.stem(word) for word in rem_punct])\n",
    "print(\"LancasterStemmer\")\n",
    "print([lancaster.stem(word) for word in rem_punct])\n",
    "print(\"SnowballStemmer\")\n",
    "print([snowball.stem(word) for word in rem_punct])\n",
    "\n",
    "# Lemmatization - bring word to root form (always have a meaning)\n",
    "print(\"\\nLemmatization\")\n",
    "print(\"--------------\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma=WordNetLemmatizer()\n",
    "print([lemma.lemmatize(word, pos=\"v\") for word in rem_punct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.** Create one python program from the following sentence.\n",
    "\n",
    "\"I love NLP, not you\"\n",
    "\n",
    "output : ['I', 'l', 'N', 'n', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'l', 'N', 'n', 'y']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\b\\w', \"I love NLP, not you\") # \\w represents a single alpha numeric character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
